{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTps9BZe0ore",
        "outputId": "ea1a554b-4f23-4368-ced0-6d91f0395e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.4-cp312-cp312-manylinux_2_28_x86_64.whl (36.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.6/36.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.4\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRV1h10s0uqE",
        "outputId": "bb8f3ad8-7755-47df-81e4-db1985e7547d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet optuna\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLo3FEgn0umv",
        "outputId": "7b6bbb62-da26-4ad7-cdce-8c8f14afe9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/413.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "%%writefile attentivefp_reviewer_compliant.py\n",
        "\"\"\"\n",
        "Reviewer-compliant AttentiveFP training script (Colab-friendly, notebook-independent)\n",
        "\n",
        "Implements reviewer-requested robustness & reproducibility:\n",
        "✅ train/val/test split (no test leakage)\n",
        "✅ random OR Murcko scaffold split option\n",
        "✅ Optuna hyperparameter tuning on VAL only\n",
        "✅ early stopping on VAL loss\n",
        "✅ repeated seeds + mean±std aggregation\n",
        "✅ y-randomization sanity check (shuffle TRAIN labels only)\n",
        "✅ imbalance sensitivity experiment via negative subsampling (inactive_ratio)\n",
        "✅ class-weighted BCE loss via pos_weight\n",
        "✅ saves split indices + per-seed metrics to disk\n",
        "\n",
        "Expected CSV columns (default):\n",
        "- SMILES\n",
        "- PUBCHEM_ACTIVITY_OUTCOME  (Active/Inactive)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import argparse\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.nn import AttentiveFP\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.utils import from_smiles\n",
        "\n",
        "import os\n",
        "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  # or \":16:8\"\n",
        "torch.use_deterministic_algorithms(True)\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        "    f1_score,\n",
        "    cohen_kappa_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        ")\n",
        "\n",
        "# RDKit scaffold split\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Reproducibility\n",
        "# -----------------------------\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "@dataclass\n",
        "class RunConfig:\n",
        "    csv_path: str\n",
        "    smiles_col: str = \"SMILES\"\n",
        "    label_col: str = \"PUBCHEM_ACTIVITY_OUTCOME\"\n",
        "    active_value: str = \"Active\"\n",
        "\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Splits\n",
        "    split_mode: str = \"random\"  # random | scaffold\n",
        "    test_frac: float = 0.20\n",
        "    val_frac: float = 0.10  # fraction of full dataset\n",
        "    seeds: List[int] = None\n",
        "\n",
        "    # Training\n",
        "    batch_size: int = 64\n",
        "    max_epochs: int = 75\n",
        "    patience: int = 15\n",
        "\n",
        "    # Optuna\n",
        "    do_optuna: bool = False\n",
        "    optuna_trials: int = 20\n",
        "    trial_max_epochs: int = 25\n",
        "\n",
        "    # Sanity / robustness\n",
        "    y_randomization: bool = False\n",
        "\n",
        "    # Imbalance experiment: keep all actives, subsample inactives to approx 1:k\n",
        "    inactive_ratio: Optional[int] = None\n",
        "\n",
        "    save_dir: str = \"runs_attentivefp\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Data utilities\n",
        "# -----------------------------\n",
        "def to_binary_label(x: str, active_value: str) -> int:\n",
        "    return 1 if str(x).strip() == active_value else 0\n",
        "\n",
        "\n",
        "def murcko_scaffold(smiles: str) -> str:\n",
        "    \"\"\"Return Murcko scaffold SMILES; empty string if invalid.\"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return \"\"\n",
        "        scaf = MurckoScaffold.GetScaffoldForMol(mol)\n",
        "        if scaf is None:\n",
        "            return \"\"\n",
        "        return Chem.MolToSmiles(scaf, isomericSmiles=False)\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def random_stratified_split(\n",
        "    labels: List[int],\n",
        "    test_frac: float,\n",
        "    val_frac: float,\n",
        "    seed: int,\n",
        ") -> Tuple[List[int], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Stratified split by class, returns train_idx, val_idx, test_idx over indices [0..N-1].\n",
        "    val_frac and test_frac are fractions of full dataset.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    labels_arr = np.asarray(labels, dtype=int)\n",
        "\n",
        "    pos_idx = np.where(labels_arr == 1)[0].tolist()\n",
        "    neg_idx = np.where(labels_arr == 0)[0].tolist()\n",
        "    rng.shuffle(pos_idx)\n",
        "    rng.shuffle(neg_idx)\n",
        "\n",
        "    def split_class(arr: List[int]) -> Tuple[List[int], List[int], List[int]]:\n",
        "        n = len(arr)\n",
        "        n_test = int(round(test_frac * n))\n",
        "        n_val = int(round(val_frac * n))\n",
        "        test = arr[:n_test]\n",
        "        val = arr[n_test : n_test + n_val]\n",
        "        train = arr[n_test + n_val :]\n",
        "        return train, val, test\n",
        "\n",
        "    pos_train, pos_val, pos_test = split_class(pos_idx)\n",
        "    neg_train, neg_val, neg_test = split_class(neg_idx)\n",
        "\n",
        "    train_idx = pos_train + neg_train\n",
        "    val_idx = pos_val + neg_val\n",
        "    test_idx = pos_test + neg_test\n",
        "\n",
        "    rng.shuffle(train_idx)\n",
        "    rng.shuffle(val_idx)\n",
        "    rng.shuffle(test_idx)\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "def stratified_group_split(\n",
        "    groups: List[str],\n",
        "    labels: List[int],\n",
        "    test_frac: float,\n",
        "    val_frac: float,\n",
        "    seed: int,\n",
        ") -> Tuple[List[int], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Greedy scaffold-group split approximating stratification.\n",
        "\n",
        "    groups: scaffold id per sample (len N)\n",
        "    labels: 0/1 per sample\n",
        "    test_frac/val_frac: fractions of full dataset\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    N = len(labels)\n",
        "    labels_arr = np.asarray(labels, dtype=int)\n",
        "\n",
        "    # group -> indices\n",
        "    group_to_idx: Dict[str, List[int]] = {}\n",
        "    for i, g in enumerate(groups):\n",
        "        group_to_idx.setdefault(g, []).append(i)\n",
        "\n",
        "    # shuffle groups\n",
        "    all_groups = list(group_to_idx.keys())\n",
        "    rng.shuffle(all_groups)\n",
        "\n",
        "    # desired sizes\n",
        "    n_test_target = int(round(test_frac * N))\n",
        "    n_val_target = int(round(val_frac * N))\n",
        "\n",
        "    test_idx, val_idx, train_idx = [], [], []\n",
        "\n",
        "    # Greedy fill test, then val, rest train\n",
        "    for g in all_groups:\n",
        "        idxs = group_to_idx[g]\n",
        "        # assign to test if still room, else to val if room, else train\n",
        "        if len(test_idx) + len(idxs) <= n_test_target:\n",
        "            test_idx.extend(idxs)\n",
        "        elif len(val_idx) + len(idxs) <= n_val_target:\n",
        "            val_idx.extend(idxs)\n",
        "        else:\n",
        "            train_idx.extend(idxs)\n",
        "\n",
        "    # If we underfilled test/val (can happen), move from train\n",
        "    def move_from_train(dst: List[int], n_target: int):\n",
        "        nonlocal train_idx\n",
        "        while len(dst) < n_target and len(train_idx) > 0:\n",
        "            dst.append(train_idx.pop())\n",
        "\n",
        "    move_from_train(test_idx, n_test_target)\n",
        "    move_from_train(val_idx, n_val_target)\n",
        "\n",
        "    rng.shuffle(train_idx)\n",
        "    rng.shuffle(val_idx)\n",
        "    rng.shuffle(test_idx)\n",
        "    return train_idx, val_idx, test_idx\n",
        "\n",
        "\n",
        "def apply_inactive_ratio_sampling(\n",
        "    df: pd.DataFrame,\n",
        "    labels: List[int],\n",
        "    inactive_ratio: int,\n",
        "    seed: int,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Keep all actives; subsample inactives so that #inactive ≈ #active * inactive_ratio.\n",
        "    NOTE: This is NOT oversampling. It's negative subsampling for an imbalance-stress experiment.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    labels_arr = np.asarray(labels, dtype=int)\n",
        "\n",
        "    act_idx = np.where(labels_arr == 1)[0]\n",
        "    inact_idx = np.where(labels_arr == 0)[0]\n",
        "\n",
        "    n_act = len(act_idx)\n",
        "    n_inact_target = min(len(inact_idx), n_act * inactive_ratio)\n",
        "\n",
        "    sampled_inact = rng.choice(inact_idx, size=n_inact_target, replace=False)\n",
        "    keep_idx = np.concatenate([act_idx, sampled_inact])\n",
        "    rng.shuffle(keep_idx)\n",
        "\n",
        "    return df.iloc[keep_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def build_graphs_from_smiles(\n",
        "    smiles_list: List[str],\n",
        "    labels: List[int],\n",
        ") -> Tuple[List[torch.Tensor], List[int], List[int]]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      graphs: list of torch_geometric Data objects\n",
        "      kept_indices: indices kept (valid SMILES converted)\n",
        "      skipped_indices: indices skipped (invalid SMILES)\n",
        "    \"\"\"\n",
        "    graphs = []\n",
        "    kept = []\n",
        "    skipped = []\n",
        "\n",
        "    for i, smi in enumerate(smiles_list):\n",
        "        try:\n",
        "            g = from_smiles(smi)\n",
        "            if g is None or g.x is None or g.edge_index is None:\n",
        "                skipped.append(i)\n",
        "                continue\n",
        "\n",
        "            # ensure floats\n",
        "            g.x = g.x.float()\n",
        "            if getattr(g, \"edge_attr\", None) is not None:\n",
        "                g.edge_attr = g.edge_attr.float()\n",
        "\n",
        "            # label tensor [1,1]\n",
        "            g.y = torch.tensor([labels[i]], dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "            graphs.append(g)\n",
        "            kept.append(i)\n",
        "        except Exception:\n",
        "            skipped.append(i)\n",
        "\n",
        "    return graphs, kept, skipped\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model + training\n",
        "# -----------------------------\n",
        "class AttentiveFPBinary(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        edge_dim: int,\n",
        "        hidden_channels: int,\n",
        "        num_layers: int,\n",
        "        num_timesteps: int,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = AttentiveFP(\n",
        "            in_channels=in_channels,\n",
        "            hidden_channels=hidden_channels,\n",
        "            out_channels=1,  # logits\n",
        "            edge_dim=edge_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_timesteps=num_timesteps,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        return self.model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device) -> Dict[str, float]:\n",
        "    model.eval()\n",
        "    y_true, y_prob = [], []\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "        logits = model(batch).view(-1)\n",
        "        probs = torch.sigmoid(logits)\n",
        "\n",
        "        y_true.extend(batch.y.view(-1).detach().cpu().numpy().tolist())\n",
        "        y_prob.extend(probs.detach().cpu().numpy().tolist())\n",
        "\n",
        "    y_true_arr = np.asarray(y_true, dtype=np.float32)\n",
        "    y_prob_arr = np.asarray(y_prob, dtype=np.float32)\n",
        "\n",
        "    y_pred = (y_prob_arr >= 0.5).astype(int)\n",
        "\n",
        "    metrics: Dict[str, float] = {}\n",
        "    metrics[\"accuracy\"] = float(accuracy_score(y_true_arr, y_pred))\n",
        "\n",
        "    # Only compute AUCs if both classes present\n",
        "    if len(np.unique(y_true_arr)) == 2:\n",
        "        metrics[\"roc_auc\"] = float(roc_auc_score(y_true_arr, y_prob_arr))\n",
        "        metrics[\"pr_auc\"] = float(average_precision_score(y_true_arr, y_prob_arr))\n",
        "    else:\n",
        "        metrics[\"roc_auc\"] = float(\"nan\")\n",
        "        metrics[\"pr_auc\"] = float(\"nan\")\n",
        "\n",
        "    metrics[\"precision\"] = float(precision_score(y_true_arr, y_pred, zero_division=0))\n",
        "    metrics[\"recall\"] = float(recall_score(y_true_arr, y_pred, zero_division=0))\n",
        "    metrics[\"f1\"] = float(f1_score(y_true_arr, y_pred, zero_division=0))\n",
        "    metrics[\"kappa\"] = float(cohen_kappa_score(y_true_arr, y_pred))\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def compute_pos_weight(train_labels: List[int], device: str) -> torch.Tensor:\n",
        "    n_pos = int(np.sum(train_labels))\n",
        "    n_neg = int(len(train_labels) - n_pos)\n",
        "    w = (n_neg / max(1, n_pos)) if n_pos > 0 else 1.0\n",
        "    return torch.tensor([w], dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "def train_with_early_stopping(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    device: str,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    max_epochs: int,\n",
        "    patience: int,\n",
        "    pos_weight: torch.Tensor,\n",
        ") -> Tuple[nn.Module, Dict[str, float]]:\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "    best_state = None\n",
        "    bad = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss, n_batches = 0.0, 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch).view(-1)\n",
        "            y = batch.y.view(-1)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += float(loss.item())\n",
        "            n_batches += 1\n",
        "\n",
        "        # validation loss\n",
        "        model.eval()\n",
        "        val_loss, val_batches = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                logits = model(batch).view(-1)\n",
        "                y = batch.y.view(-1)\n",
        "                loss = criterion(logits, y)\n",
        "                val_loss += float(loss.item())\n",
        "                val_batches += 1\n",
        "\n",
        "        val_loss = val_loss / max(1, val_batches)\n",
        "\n",
        "        if val_loss < best_val - 1e-5:\n",
        "            best_val = val_loss\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    val_metrics = evaluate(model, val_loader, device)\n",
        "    val_metrics[\"val_loss_best\"] = float(best_val)\n",
        "    return model, val_metrics\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Optuna tuning (VAL only)\n",
        "# -----------------------------\n",
        "def optuna_objective_factory(\n",
        "    graphs: List,\n",
        "    train_idx: List[int],\n",
        "    val_idx: List[int],\n",
        "    cfg: RunConfig,\n",
        "    in_channels: int,\n",
        "    edge_dim: int,\n",
        "    train_labels: List[int],\n",
        "):\n",
        "    device = cfg.device\n",
        "\n",
        "    train_set = [graphs[i] for i in train_idx]\n",
        "    val_set = [graphs[i] for i in val_idx]\n",
        "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    pos_weight = compute_pos_weight(train_labels, device=device)\n",
        "\n",
        "    def objective(trial: optuna.Trial) -> float:\n",
        "        hidden = trial.suggest_int(\"hidden_channels\", 64, 256)\n",
        "        num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
        "        num_timesteps = trial.suggest_int(\"num_timesteps\", 2, 6)\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
        "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "        wd = trial.suggest_float(\"weight_decay\", 1e-7, 1e-3, log=True)\n",
        "\n",
        "        model = AttentiveFPBinary(\n",
        "            in_channels=in_channels,\n",
        "            edge_dim=edge_dim,\n",
        "            hidden_channels=hidden,\n",
        "            num_layers=num_layers,\n",
        "            num_timesteps=num_timesteps,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "        model, _ = train_with_early_stopping(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            device=device,\n",
        "            lr=lr,\n",
        "            weight_decay=wd,\n",
        "            max_epochs=cfg.trial_max_epochs,\n",
        "            patience=max(5, cfg.patience // 2),\n",
        "            pos_weight=pos_weight,\n",
        "        )\n",
        "\n",
        "        val_metrics = evaluate(model, val_loader, device)\n",
        "        score = val_metrics.get(\"roc_auc\", float(\"nan\"))\n",
        "        if np.isnan(score):\n",
        "            score = val_metrics.get(\"accuracy\", 0.0)\n",
        "        return float(score)\n",
        "\n",
        "    return objective\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run one seed\n",
        "# -----------------------------\n",
        "def run_one_seed(cfg: RunConfig, seed: int) -> Dict[str, object]:\n",
        "    set_seed(seed)\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(cfg.save_dir, \"splits\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(cfg.save_dir, \"metrics\"), exist_ok=True)\n",
        "\n",
        "    df = pd.read_csv(cfg.csv_path)\n",
        "\n",
        "    # labels from raw df (before invalid SMILES filtering)\n",
        "    raw_labels = [to_binary_label(x, cfg.active_value) for x in df[cfg.label_col].tolist()]\n",
        "\n",
        "    # Optional imbalance sampling (keeps all actives; samples inactives)\n",
        "    if cfg.inactive_ratio is not None and cfg.inactive_ratio > 0:\n",
        "        df = apply_inactive_ratio_sampling(df, raw_labels, cfg.inactive_ratio, seed)\n",
        "        raw_labels = [to_binary_label(x, cfg.active_value) for x in df[cfg.label_col].tolist()]\n",
        "\n",
        "    smiles = df[cfg.smiles_col].astype(str).tolist()\n",
        "\n",
        "    # Build graphs (skip invalid SMILES)\n",
        "    graphs, kept, skipped = build_graphs_from_smiles(smiles, raw_labels)\n",
        "    labels_kept = [raw_labels[i] for i in kept]\n",
        "    smiles_kept = [smiles[i] for i in kept]\n",
        "\n",
        "    if len(graphs) < 50:\n",
        "        raise RuntimeError(f\"Too few valid graphs after SMILES parsing: {len(graphs)}\")\n",
        "\n",
        "    # Infer input dims\n",
        "    in_channels = int(graphs[0].x.size(-1))\n",
        "    edge_dim = int(graphs[0].edge_attr.size(-1)) if getattr(graphs[0], \"edge_attr\", None) is not None else 0\n",
        "\n",
        "    # Split indices on the kept list (0..len(graphs)-1)\n",
        "    if cfg.split_mode == \"scaffold\":\n",
        "        scaffolds = [murcko_scaffold(smi) for smi in smiles_kept]\n",
        "        scaffolds = [s if s else f\"NOSCAF_{i}\" for i, s in enumerate(scaffolds)]\n",
        "        train_idx, val_idx, test_idx = stratified_group_split(\n",
        "            groups=scaffolds,\n",
        "            labels=labels_kept,\n",
        "            test_frac=cfg.test_frac,\n",
        "            val_frac=cfg.val_frac,\n",
        "            seed=seed,\n",
        "        )\n",
        "    else:\n",
        "        train_idx, val_idx, test_idx = random_stratified_split(\n",
        "            labels=labels_kept,\n",
        "            test_frac=cfg.test_frac,\n",
        "            val_frac=cfg.val_frac,\n",
        "            seed=seed,\n",
        "        )\n",
        "\n",
        "    # y-randomization only on TRAIN labels\n",
        "    if cfg.y_randomization:\n",
        "        rng = np.random.default_rng(seed)\n",
        "        y_train = np.array([labels_kept[i] for i in train_idx], dtype=int)\n",
        "        rng.shuffle(y_train)\n",
        "        for j, idx in enumerate(train_idx):\n",
        "            graphs[idx].y = torch.tensor([int(y_train[j])], dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Dataloaders\n",
        "    train_set = [graphs[i] for i in train_idx]\n",
        "    val_set = [graphs[i] for i in val_idx]\n",
        "    test_set = [graphs[i] for i in test_idx]\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=cfg.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=cfg.batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_set, batch_size=cfg.batch_size, shuffle=False)\n",
        "\n",
        "    # Default params (will be overridden by Optuna best)\n",
        "    best_params = dict(\n",
        "        hidden_channels=128,\n",
        "        num_layers=3,\n",
        "        num_timesteps=3,\n",
        "        dropout=0.2,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-5,\n",
        "    )\n",
        "\n",
        "    # Optuna tuning on VAL only\n",
        "    if cfg.do_optuna:\n",
        "        train_labels = [labels_kept[i] for i in train_idx]\n",
        "        objective = optuna_objective_factory(\n",
        "            graphs=graphs,\n",
        "            train_idx=train_idx,\n",
        "            val_idx=val_idx,\n",
        "            cfg=cfg,\n",
        "            in_channels=in_channels,\n",
        "            edge_dim=edge_dim,\n",
        "            train_labels=train_labels,\n",
        "        )\n",
        "        sampler = optuna.samplers.TPESampler(seed=seed)\n",
        "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
        "        study.optimize(objective, n_trials=cfg.optuna_trials)\n",
        "\n",
        "        best_params.update(study.best_params)\n",
        "\n",
        "        # Some params are not in Optuna (lr, weight_decay always are; keep safe fallback)\n",
        "        if \"lr\" not in best_params:\n",
        "            best_params[\"lr\"] = 1e-3\n",
        "        if \"weight_decay\" not in best_params:\n",
        "            best_params[\"weight_decay\"] = 1e-5\n",
        "\n",
        "    # Train final model with best params\n",
        "    model = AttentiveFPBinary(\n",
        "        in_channels=in_channels,\n",
        "        edge_dim=edge_dim,\n",
        "        hidden_channels=int(best_params[\"hidden_channels\"]),\n",
        "        num_layers=int(best_params[\"num_layers\"]),\n",
        "        num_timesteps=int(best_params[\"num_timesteps\"]),\n",
        "        dropout=float(best_params[\"dropout\"]),\n",
        "    )\n",
        "\n",
        "    train_labels = [labels_kept[i] for i in train_idx]\n",
        "    pos_weight = compute_pos_weight(train_labels, device=cfg.device)\n",
        "\n",
        "    model, val_metrics = train_with_early_stopping(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        device=cfg.device,\n",
        "        lr=float(best_params[\"lr\"]),\n",
        "        weight_decay=float(best_params[\"weight_decay\"]),\n",
        "        max_epochs=cfg.max_epochs,\n",
        "        patience=cfg.patience,\n",
        "        pos_weight=pos_weight,\n",
        "    )\n",
        "\n",
        "    test_metrics = evaluate(model, test_loader, cfg.device)\n",
        "\n",
        "    # Save splits\n",
        "    split_payload = {\n",
        "        \"seed\": seed,\n",
        "        \"csv_path\": cfg.csv_path,\n",
        "        \"split_mode\": cfg.split_mode,\n",
        "        \"test_frac\": cfg.test_frac,\n",
        "        \"val_frac\": cfg.val_frac,\n",
        "        \"train_idx\": train_idx,\n",
        "        \"val_idx\": val_idx,\n",
        "        \"test_idx\": test_idx,\n",
        "        \"n_graphs\": len(graphs),\n",
        "        \"skipped_smiles_count\": len(skipped),\n",
        "        \"in_channels\": in_channels,\n",
        "        \"edge_dim\": edge_dim,\n",
        "        \"y_randomization\": cfg.y_randomization,\n",
        "        \"inactive_ratio\": cfg.inactive_ratio,\n",
        "    }\n",
        "    split_path = os.path.join(cfg.save_dir, \"splits\", f\"splits_seed{seed}.json\")\n",
        "    with open(split_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(split_payload, f, indent=2)\n",
        "\n",
        "    # Save metrics + params\n",
        "    result = {\n",
        "        \"seed\": seed,\n",
        "        \"best_params\": best_params,\n",
        "        \"val_metrics\": val_metrics,\n",
        "        \"test_metrics\": test_metrics,\n",
        "        \"split_path\": split_path,\n",
        "    }\n",
        "    metrics_path = os.path.join(cfg.save_dir, \"metrics\", f\"metrics_seed{seed}.json\")\n",
        "    with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def aggregate_results(results: List[Dict[str, object]]) -> Dict[str, Dict[str, float]]:\n",
        "    keys = [\"accuracy\", \"roc_auc\", \"pr_auc\", \"precision\", \"recall\", \"f1\", \"kappa\"]\n",
        "    data = {k: [] for k in keys}\n",
        "    for r in results:\n",
        "        tm = r[\"test_metrics\"]\n",
        "        for k in keys:\n",
        "            data[k].append(tm.get(k, float(\"nan\")))\n",
        "\n",
        "    summary = {}\n",
        "    for k in keys:\n",
        "        arr = np.asarray(data[k], dtype=np.float64)\n",
        "        summary[k] = {\n",
        "            \"mean\": float(np.nanmean(arr)),\n",
        "            \"std\": float(np.nanstd(arr, ddof=1)) if np.sum(~np.isnan(arr)) > 1 else float(\"nan\"),\n",
        "        }\n",
        "    return {\"test_summary\": summary}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# CLI\n",
        "# -----------------------------\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--csv_path\", type=str, required=True, help=\"Path to AID*.csv file with SMILES + labels\")\n",
        "    p.add_argument(\"--split_mode\", type=str, default=\"random\", choices=[\"random\", \"scaffold\"])\n",
        "    p.add_argument(\"--seeds\", type=str, default=\"0,1,2,3,4\", help=\"Comma-separated seeds\")\n",
        "    p.add_argument(\"--save_dir\", type=str, default=\"runs_attentivefp\")\n",
        "\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--max_epochs\", type=int, default=75)\n",
        "    p.add_argument(\"--patience\", type=int, default=15)\n",
        "\n",
        "    p.add_argument(\"--do_optuna\", action=\"store_true\", help=\"Enable Optuna tuning (VAL only)\")\n",
        "    p.add_argument(\"--optuna_trials\", type=int, default=20)\n",
        "    p.add_argument(\"--trial_max_epochs\", type=int, default=25)\n",
        "\n",
        "    p.add_argument(\"--split_test_frac\", type=float, default=0.20)\n",
        "    p.add_argument(\"--split_val_frac\", type=float, default=0.10)\n",
        "\n",
        "    p.add_argument(\"--y_randomization\", action=\"store_true\", help=\"Shuffle TRAIN labels for sanity check\")\n",
        "    p.add_argument(\"--inactive_ratio\", type=int, default=None, help=\"If set, sample inactives to approx 1:k ratio\")\n",
        "\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    cfg = RunConfig(\n",
        "        csv_path=args.csv_path,\n",
        "        split_mode=args.split_mode,\n",
        "        test_frac=args.split_test_frac,\n",
        "        val_frac=args.split_val_frac,\n",
        "        seeds=[int(x.strip()) for x in args.seeds.split(\",\") if x.strip() != \"\"],\n",
        "        save_dir=args.save_dir,\n",
        "        batch_size=args.batch_size,\n",
        "        max_epochs=args.max_epochs,\n",
        "        patience=args.patience,\n",
        "        do_optuna=args.do_optuna,\n",
        "        optuna_trials=args.optuna_trials,\n",
        "        trial_max_epochs=args.trial_max_epochs,\n",
        "        y_randomization=args.y_randomization,\n",
        "        inactive_ratio=args.inactive_ratio,\n",
        "    )\n",
        "\n",
        "    os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Device: {cfg.device}\")\n",
        "    print(f\"CSV: {cfg.csv_path}\")\n",
        "    print(f\"Split: {cfg.split_mode} | test_frac={cfg.test_frac} val_frac={cfg.val_frac}\")\n",
        "    print(f\"Seeds: {cfg.seeds}\")\n",
        "    print(f\"Optuna: {cfg.do_optuna} | trials={cfg.optuna_trials}\")\n",
        "    print(f\"y_randomization: {cfg.y_randomization} | inactive_ratio: {cfg.inactive_ratio}\")\n",
        "\n",
        "    all_results = []\n",
        "    for seed in cfg.seeds:\n",
        "        print(f\"\\n=== Running seed {seed} ===\")\n",
        "        res = run_one_seed(cfg, seed)\n",
        "        print(\"Test metrics:\", res[\"test_metrics\"])\n",
        "        all_results.append(res)\n",
        "\n",
        "    summary = aggregate_results(all_results)\n",
        "    summary_path = os.path.join(cfg.save_dir, \"summary_mean_std.json\")\n",
        "    with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, indent=2)\n",
        "\n",
        "    print(\"\\n=== Mean±Std (TEST) ===\")\n",
        "    for k, v in summary[\"test_summary\"].items():\n",
        "        print(f\"{k:>10}: {v['mean']:.4f} ± {v['std']:.4f}\")\n",
        "    print(f\"\\nSaved summary: {summary_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHabcMKM0ui-",
        "outputId": "d177d412-0b8d-4489-83c5-151b9349a998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing attentivefp_reviewer_compliant.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eYI5DEn50ubz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "83YG8tNkVCI8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}