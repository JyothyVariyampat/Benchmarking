{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwSghrJi8yqn",
        "outputId": "ad0e9b63-56b1-4153-e9bb-fdc9c569e417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.3-cp312-cp312-manylinux_2_28_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.3\n"
          ]
        }
      ],
      "source": [
        "!pip install rdkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlUPhHrd8yRE",
        "outputId": "840168fc-f03c-425a-c878-a99e02ae3b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc6CRlNy8yH7",
        "outputId": "8ad3a677-071e-4f54-d5df-579c34196c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/404.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyLXkr6e8sWP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPeZlmkHC-R4"
      },
      "outputs": [],
      "source": [
        "# above code with optuna integrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMedBF6SEDY2",
        "outputId": "1f828fcc-1461-4fad-ecfa-5bfe8a611711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total valid molecules: 4000\n",
            "in_channels=9, edge_dim=3\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-175813469.py:86: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
            "/tmp/ipython-input-175813469.py:87: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Train Loss: 0.6293, Train ROC-AUC: 0.7037 | Test Loss: 0.6428, Test ROC-AUC: 0.7877, F1: 0.7196, Acc: 0.6162, Prec: 0.5735, Rec: 0.9657\n",
            "Test Confusion Matrix:\n",
            " [[ 99 293]\n",
            " [ 14 394]]\n",
            "Epoch 002 | Train Loss: 0.5904, Train ROC-AUC: 0.7523 | Test Loss: 0.5561, Test ROC-AUC: 0.7948, F1: 0.7439, Acc: 0.7375, Prec: 0.7403, Rec: 0.7475\n",
            "Test Confusion Matrix:\n",
            " [[285 107]\n",
            " [103 305]]\n",
            "Epoch 003 | Train Loss: 0.5698, Train ROC-AUC: 0.7747 | Test Loss: 0.5487, Test ROC-AUC: 0.7999, F1: 0.7520, Acc: 0.7238, Prec: 0.6936, Rec: 0.8211\n",
            "Test Confusion Matrix:\n",
            " [[244 148]\n",
            " [ 73 335]]\n",
            "Epoch 004 | Train Loss: 0.5604, Train ROC-AUC: 0.7833 | Test Loss: 0.5654, Test ROC-AUC: 0.8018, F1: 0.6938, Acc: 0.7175, Prec: 0.7758, Rec: 0.6275\n",
            "Test Confusion Matrix:\n",
            " [[318  74]\n",
            " [152 256]]\n",
            "Epoch 005 | Train Loss: 0.5527, Train ROC-AUC: 0.7932 | Test Loss: 0.5349, Test ROC-AUC: 0.8143, F1: 0.7646, Acc: 0.7475, Prec: 0.7289, Rec: 0.8039\n",
            "Test Confusion Matrix:\n",
            " [[270 122]\n",
            " [ 80 328]]\n",
            "Epoch 006 | Train Loss: 0.5462, Train ROC-AUC: 0.7982 | Test Loss: 0.5465, Test ROC-AUC: 0.8116, F1: 0.7672, Acc: 0.7375, Prec: 0.7004, Rec: 0.8480\n",
            "Test Confusion Matrix:\n",
            " [[244 148]\n",
            " [ 62 346]]\n",
            "Epoch 007 | Train Loss: 0.5391, Train ROC-AUC: 0.8052 | Test Loss: 0.5668, Test ROC-AUC: 0.8146, F1: 0.6835, Acc: 0.7188, Prec: 0.8020, Rec: 0.5956\n",
            "Test Confusion Matrix:\n",
            " [[332  60]\n",
            " [165 243]]\n",
            "Epoch 008 | Train Loss: 0.5297, Train ROC-AUC: 0.8135 | Test Loss: 0.5219, Test ROC-AUC: 0.8220, F1: 0.7698, Acc: 0.7675, Prec: 0.7775, Rec: 0.7623\n",
            "Test Confusion Matrix:\n",
            " [[303  89]\n",
            " [ 97 311]]\n",
            "Epoch 009 | Train Loss: 0.5314, Train ROC-AUC: 0.8121 | Test Loss: 0.5356, Test ROC-AUC: 0.8285, F1: 0.7627, Acc: 0.7612, Prec: 0.7733, Rec: 0.7525\n",
            "Test Confusion Matrix:\n",
            " [[302  90]\n",
            " [101 307]]\n",
            "Epoch 010 | Train Loss: 0.5146, Train ROC-AUC: 0.8258 | Test Loss: 0.5236, Test ROC-AUC: 0.8248, F1: 0.7763, Acc: 0.7500, Prec: 0.7140, Rec: 0.8505\n",
            "Test Confusion Matrix:\n",
            " [[253 139]\n",
            " [ 61 347]]\n",
            "Epoch 011 | Train Loss: 0.5217, Train ROC-AUC: 0.8200 | Test Loss: 0.5118, Test ROC-AUC: 0.8293, F1: 0.7687, Acc: 0.7638, Prec: 0.7677, Rec: 0.7696\n",
            "Test Confusion Matrix:\n",
            " [[297  95]\n",
            " [ 94 314]]\n",
            "Epoch 012 | Train Loss: 0.5147, Train ROC-AUC: 0.8252 | Test Loss: 0.5366, Test ROC-AUC: 0.8205, F1: 0.7249, Acc: 0.7362, Prec: 0.7744, Rec: 0.6814\n",
            "Test Confusion Matrix:\n",
            " [[311  81]\n",
            " [130 278]]\n",
            "Epoch 013 | Train Loss: 0.5085, Train ROC-AUC: 0.8301 | Test Loss: 0.5090, Test ROC-AUC: 0.8319, F1: 0.7541, Acc: 0.7538, Prec: 0.7684, Rec: 0.7402\n",
            "Test Confusion Matrix:\n",
            " [[301  91]\n",
            " [106 302]]\n",
            "Epoch 014 | Train Loss: 0.5030, Train ROC-AUC: 0.8350 | Test Loss: 0.5259, Test ROC-AUC: 0.8407, F1: 0.7870, Acc: 0.7550, Prec: 0.7070, Rec: 0.8873\n",
            "Test Confusion Matrix:\n",
            " [[242 150]\n",
            " [ 46 362]]\n",
            "Epoch 015 | Train Loss: 0.5149, Train ROC-AUC: 0.8254 | Test Loss: 0.5213, Test ROC-AUC: 0.8361, F1: 0.7221, Acc: 0.7412, Prec: 0.7982, Rec: 0.6593\n",
            "Test Confusion Matrix:\n",
            " [[324  68]\n",
            " [139 269]]\n",
            "Epoch 016 | Train Loss: 0.5022, Train ROC-AUC: 0.8350 | Test Loss: 0.5072, Test ROC-AUC: 0.8311, F1: 0.7659, Acc: 0.7562, Prec: 0.7506, Rec: 0.7819\n",
            "Test Confusion Matrix:\n",
            " [[286 106]\n",
            " [ 89 319]]\n",
            "Epoch 017 | Train Loss: 0.5006, Train ROC-AUC: 0.8370 | Test Loss: 0.5380, Test ROC-AUC: 0.8343, F1: 0.7248, Acc: 0.7475, Prec: 0.8160, Rec: 0.6520\n",
            "Test Confusion Matrix:\n",
            " [[332  60]\n",
            " [142 266]]\n",
            "Epoch 018 | Train Loss: 0.4958, Train ROC-AUC: 0.8408 | Test Loss: 0.5051, Test ROC-AUC: 0.8348, F1: 0.7656, Acc: 0.7650, Prec: 0.7792, Rec: 0.7525\n",
            "Test Confusion Matrix:\n",
            " [[305  87]\n",
            " [101 307]]\n",
            "Epoch 019 | Train Loss: 0.5004, Train ROC-AUC: 0.8371 | Test Loss: 0.5347, Test ROC-AUC: 0.8349, F1: 0.7373, Acc: 0.7550, Prec: 0.8136, Rec: 0.6740\n",
            "Test Confusion Matrix:\n",
            " [[329  63]\n",
            " [133 275]]\n",
            "Epoch 020 | Train Loss: 0.4990, Train ROC-AUC: 0.8376 | Test Loss: 0.5300, Test ROC-AUC: 0.8336, F1: 0.7258, Acc: 0.7488, Prec: 0.8185, Rec: 0.6520\n",
            "Test Confusion Matrix:\n",
            " [[333  59]\n",
            " [142 266]]\n",
            "Epoch 021 | Train Loss: 0.4924, Train ROC-AUC: 0.8427 | Test Loss: 0.4938, Test ROC-AUC: 0.8420, F1: 0.7700, Acc: 0.7700, Prec: 0.7857, Rec: 0.7549\n",
            "Test Confusion Matrix:\n",
            " [[308  84]\n",
            " [100 308]]\n",
            "Epoch 022 | Train Loss: 0.4848, Train ROC-AUC: 0.8477 | Test Loss: 0.4972, Test ROC-AUC: 0.8403, F1: 0.7811, Acc: 0.7688, Prec: 0.7551, Rec: 0.8088\n",
            "Test Confusion Matrix:\n",
            " [[285 107]\n",
            " [ 78 330]]\n",
            "Epoch 023 | Train Loss: 0.4868, Train ROC-AUC: 0.8473 | Test Loss: 0.4966, Test ROC-AUC: 0.8464, F1: 0.7490, Acc: 0.7588, Prec: 0.7978, Rec: 0.7059\n",
            "Test Confusion Matrix:\n",
            " [[319  73]\n",
            " [120 288]]\n",
            "Epoch 024 | Train Loss: 0.4851, Train ROC-AUC: 0.8473 | Test Loss: 0.4880, Test ROC-AUC: 0.8479, F1: 0.7748, Acc: 0.7725, Prec: 0.7825, Rec: 0.7672\n",
            "Test Confusion Matrix:\n",
            " [[305  87]\n",
            " [ 95 313]]\n",
            "Epoch 025 | Train Loss: 0.4780, Train ROC-AUC: 0.8533 | Test Loss: 0.4999, Test ROC-AUC: 0.8382, F1: 0.7669, Acc: 0.7712, Prec: 0.7984, Rec: 0.7377\n",
            "Test Confusion Matrix:\n",
            " [[316  76]\n",
            " [107 301]]\n",
            "Epoch 026 | Train Loss: 0.4789, Train ROC-AUC: 0.8525 | Test Loss: 0.4936, Test ROC-AUC: 0.8469, F1: 0.7891, Acc: 0.7775, Prec: 0.7638, Rec: 0.8162\n",
            "Test Confusion Matrix:\n",
            " [[289 103]\n",
            " [ 75 333]]\n",
            "Epoch 027 | Train Loss: 0.4848, Train ROC-AUC: 0.8475 | Test Loss: 0.4961, Test ROC-AUC: 0.8416, F1: 0.7739, Acc: 0.7662, Prec: 0.7637, Rec: 0.7843\n",
            "Test Confusion Matrix:\n",
            " [[293  99]\n",
            " [ 88 320]]\n",
            "Epoch 028 | Train Loss: 0.4861, Train ROC-AUC: 0.8469 | Test Loss: 0.4945, Test ROC-AUC: 0.8434, F1: 0.7774, Acc: 0.7788, Prec: 0.7984, Rec: 0.7574\n",
            "Test Confusion Matrix:\n",
            " [[314  78]\n",
            " [ 99 309]]\n",
            "Epoch 029 | Train Loss: 0.4792, Train ROC-AUC: 0.8520 | Test Loss: 0.5170, Test ROC-AUC: 0.8399, F1: 0.7270, Acc: 0.7512, Prec: 0.8255, Rec: 0.6495\n",
            "Test Confusion Matrix:\n",
            " [[336  56]\n",
            " [143 265]]\n",
            "Epoch 030 | Train Loss: 0.4777, Train ROC-AUC: 0.8538 | Test Loss: 0.4937, Test ROC-AUC: 0.8445, F1: 0.7885, Acc: 0.7788, Prec: 0.7692, Rec: 0.8088\n",
            "Test Confusion Matrix:\n",
            " [[293  99]\n",
            " [ 78 330]]\n",
            "\n",
            "Best Test ROC-AUC (AttentiveFP-only): 0.8479266706682673\n",
            "\n",
            "Extracting embeddings for train and test sets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 17:17:09,042] A new study created in memory with name: no-name-c4dadd57-1aad-4145-816c-8d0897b07856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding shapes: (3200, 128) (800, 128)\n",
            "\n",
            "Running Optuna hyperparameter search for SVM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-05 17:17:10,813] Trial 0 finished with value: 0.8685788507363568 and parameters: {'C': 0.10019800923586661, 'gamma': 0.0021494938298300325, 'kernel': 'linear'}. Best is trial 0 with value: 0.8685788507363568.\n",
            "[I 2025-12-05 17:17:12,522] Trial 1 finished with value: 0.8684030626196335 and parameters: {'C': 0.09180941338464514, 'gamma': 0.6012137094838448, 'kernel': 'linear'}. Best is trial 0 with value: 0.8685788507363568.\n",
            "[I 2025-12-05 17:17:14,523] Trial 2 finished with value: 0.8534708387046369 and parameters: {'C': 0.6936851225123591, 'gamma': 0.004545197215670044, 'kernel': 'rbf'}. Best is trial 0 with value: 0.8685788507363568.\n",
            "[I 2025-12-05 17:17:16,650] Trial 3 finished with value: 0.8714842376655338 and parameters: {'C': 0.4017557334496838, 'gamma': 0.00017491547783003838, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:40:36,671] Trial 4 finished with value: 0.867817102230556 and parameters: {'C': 589.8714421965096, 'gamma': 0.00010197211751660892, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:42:39,115] Trial 5 finished with value: 0.8682565725223641 and parameters: {'C': 51.94942343177747, 'gamma': 0.06764763540433034, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:42:42,629] Trial 6 finished with value: 0.8144068127661236 and parameters: {'C': 0.02091742088604308, 'gamma': 0.09159971110524882, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:42:46,293] Trial 7 finished with value: 0.7913395054494318 and parameters: {'C': 27.359092207264386, 'gamma': 0.15691544267141821, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:42:51,454] Trial 8 finished with value: 0.8710545333802102 and parameters: {'C': 1.4031662858635188, 'gamma': 0.03382104385117001, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:42:54,966] Trial 9 finished with value: 0.8432555959217157 and parameters: {'C': 104.06421403112716, 'gamma': 0.006063456595560113, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:42:59,369] Trial 10 finished with value: 0.7156431891870777 and parameters: {'C': 2.8379222213495408, 'gamma': 5.542677443063308, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:04,873] Trial 11 finished with value: 0.8711668424547834 and parameters: {'C': 1.8138706943824763, 'gamma': 0.00023235747892530063, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:07,202] Trial 12 finished with value: 0.869325950232431 and parameters: {'C': 0.33554970741689183, 'gamma': 0.00012129765825599068, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:09,160] Trial 13 finished with value: 0.866654947458885 and parameters: {'C': 9.030736817826687, 'gamma': 0.0006402688414971068, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:11,234] Trial 14 finished with value: 0.869418727294035 and parameters: {'C': 0.23366093947851674, 'gamma': 0.0005472343984697001, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:13,932] Trial 15 finished with value: 0.8698142505566623 and parameters: {'C': 0.013430761051956373, 'gamma': 0.0007034307131095599, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:16,267] Trial 16 finished with value: 0.8429723817336615 and parameters: {'C': 8.21215410628495, 'gamma': 0.010175543434253155, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:28,680] Trial 17 finished with value: 0.8710252353607563 and parameters: {'C': 5.050528541338048, 'gamma': 0.0003284939653860642, 'kernel': 'linear'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:30,766] Trial 18 finished with value: 0.8631880151568422 and parameters: {'C': 0.046425497815070285, 'gamma': 0.0019419136848502557, 'kernel': 'rbf'}. Best is trial 3 with value: 0.8714842376655338.\n",
            "[I 2025-12-05 17:43:34,792] Trial 19 finished with value: 0.8716795577952264 and parameters: {'C': 1.0082850426260872, 'gamma': 0.00020982602613067917, 'kernel': 'linear'}. Best is trial 19 with value: 0.8716795577952264.\n",
            "[I 2025-12-05 17:43:37,806] Trial 20 finished with value: 0.8696628774561507 and parameters: {'C': 0.5853176387972983, 'gamma': 0.0015647382977761082, 'kernel': 'linear'}. Best is trial 19 with value: 0.8716795577952264.\n",
            "[I 2025-12-05 17:43:43,049] Trial 21 finished with value: 0.8711863744677526 and parameters: {'C': 1.7289281576190951, 'gamma': 0.00017000302911310343, 'kernel': 'linear'}. Best is trial 19 with value: 0.8716795577952264.\n",
            "[I 2025-12-05 17:43:47,273] Trial 22 finished with value: 0.8717088558146802 and parameters: {'C': 1.0345378556644573, 'gamma': 0.0002250641138724055, 'kernel': 'linear'}. Best is trial 22 with value: 0.8717088558146802.\n",
            "[I 2025-12-05 17:43:49,201] Trial 23 finished with value: 0.8693405992421579 and parameters: {'C': 0.1852252666515261, 'gamma': 0.0011349957312135977, 'kernel': 'linear'}. Best is trial 22 with value: 0.8717088558146802.\n",
            "[I 2025-12-05 17:43:53,209] Trial 24 finished with value: 0.8716502597757725 and parameters: {'C': 1.031897226623, 'gamma': 0.0003166827314759431, 'kernel': 'linear'}. Best is trial 22 with value: 0.8717088558146802.\n",
            "[I 2025-12-05 17:44:33,669] Trial 25 finished with value: 0.8698093675534201 and parameters: {'C': 19.27397883556091, 'gamma': 0.009510746015916734, 'kernel': 'linear'}. Best is trial 22 with value: 0.8717088558146802.\n",
            "[I 2025-12-05 17:44:37,833] Trial 26 finished with value: 0.8719383569670691 and parameters: {'C': 1.143774217375179, 'gamma': 0.0004116943377543645, 'kernel': 'linear'}. Best is trial 26 with value: 0.8719383569670691.\n",
            "[I 2025-12-05 17:44:46,845] Trial 27 finished with value: 0.8700291026993242 and parameters: {'C': 3.6095255849740577, 'gamma': 1.2249543226956419, 'kernel': 'linear'}. Best is trial 26 with value: 0.8719383569670691.\n",
            "[I 2025-12-05 17:44:48,419] Trial 28 finished with value: 0.8684714246650259 and parameters: {'C': 0.08976412022942484, 'gamma': 0.003414861916670972, 'kernel': 'linear'}. Best is trial 26 with value: 0.8719383569670691.\n",
            "[I 2025-12-05 17:44:49,869] Trial 29 finished with value: 0.8692722371967655 and parameters: {'C': 0.0507269029651512, 'gamma': 0.000931700657415349, 'kernel': 'linear'}. Best is trial 26 with value: 0.8719383569670691.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best SVM params: {'C': 1.143774217375179, 'gamma': 0.0004116943377543645, 'kernel': 'linear'}\n",
            "Best validation ROC-AUC from Optuna: 0.8719383569670691\n",
            "\n",
            "Training final SVM with best hyperparameters on full training embeddings...\n",
            "\n",
            "===== SVM on AttentiveFP Embeddings (Optuna-tuned) =====\n",
            "ROC-AUC   : 0.8444\n",
            "F1        : 0.7760\n",
            "Accuracy  : 0.7762\n",
            "Kappa     : 0.5527\n",
            "Precision : 0.7928\n",
            "Recall    : 0.7598\n",
            "Confusion Matrix:\n",
            " [[311  81]\n",
            " [ 98 310]]\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# Trial-2: AttentiveFP → Embeddings → SVM+Optuna\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import os\n",
        "\n",
        "from torch_geometric.utils import from_smiles\n",
        "from torch_geometric.data import DataLoader\n",
        "from torch_geometric.nn import AttentiveFP\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, f1_score, cohen_kappa_score,\n",
        "    precision_score, recall_score, accuracy_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import optuna\n",
        "\n",
        "# ------------------------\n",
        "# Reproducibility\n",
        "# ------------------------\n",
        "def seed_set(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_set(42)\n",
        "\n",
        "# ------------------------\n",
        "# Load dataset\n",
        "# ------------------------\n",
        "# Adjust path as needed\n",
        "df = pd.read_csv('/content/AID1239_data_for_classification_04Nov2024.csv')\n",
        "\n",
        "# Optional: limit data for quick tests\n",
        "# df = df.head(20000)\n",
        "\n",
        "# ------------------------\n",
        "# SMILES → Graph list\n",
        "# ------------------------\n",
        "graph_list = []\n",
        "for i, smile in enumerate(df['SMILES']):\n",
        "    try:\n",
        "        g = from_smiles(smile)\n",
        "        g.x = g.x.float()\n",
        "        y_value = 1.0 if df['PUBCHEM_ACTIVITY_OUTCOME'][i] == 'Active' else 0.0\n",
        "        y = torch.tensor(y_value, dtype=torch.float).view(1, -1)  # shape [1,1]\n",
        "        g.y = y\n",
        "        graph_list.append(g)\n",
        "    except Exception:\n",
        "        # Skip SMILES that fail to parse\n",
        "        continue\n",
        "\n",
        "print(f\"Total valid molecules: {len(graph_list)}\")\n",
        "\n",
        "# Infer feature sizes from first graph\n",
        "in_channels = graph_list[0].x.size(-1)\n",
        "edge_dim = graph_list[0].edge_attr.size(-1)\n",
        "print(f\"in_channels={in_channels}, edge_dim={edge_dim}\")\n",
        "\n",
        "# ------------------------\n",
        "# Train / Test split  (random; you can later replace with scaffold split)\n",
        "# ------------------------\n",
        "train_size = int(0.8 * len(graph_list))\n",
        "test_size = len(graph_list) - train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(\n",
        "    graph_list,\n",
        "    [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# ------------------------\n",
        "# Device\n",
        "# ------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# =========================================\n",
        "# AttentiveFP Encoder + Linear Classifier\n",
        "# =========================================\n",
        "class AttentiveFPEncoderClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Uses AttentiveFP as a graph encoder (outputs embedding of size 'hidden_channels'),\n",
        "    then applies a Linear layer for classification.\n",
        "    We can extract the embedding for SVM later.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, edge_dim,\n",
        "                 hidden_channels=128, num_layers=4,\n",
        "                 num_timesteps=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # AttentiveFP will output embeddings of size hidden_channels\n",
        "        self.encoder = AttentiveFP(\n",
        "            in_channels=in_channels,\n",
        "            hidden_channels=hidden_channels,\n",
        "            out_channels=hidden_channels,  # embedding dimension\n",
        "            edge_dim=edge_dim,\n",
        "            num_layers=num_layers,\n",
        "            num_timesteps=num_timesteps,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_channels, 1)  # binary logit\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr, batch, return_emb: bool = False):\n",
        "        emb = self.encoder(x, edge_index, edge_attr, batch)  # [B, hidden_channels]\n",
        "        logits = self.classifier(emb)                        # [B, 1]\n",
        "        if return_emb:\n",
        "            return logits, emb\n",
        "        return logits\n",
        "\n",
        "# ------------------------\n",
        "# Instantiate model\n",
        "# ------------------------\n",
        "hidden_channels = 128\n",
        "num_layers = 4\n",
        "num_timesteps = 2\n",
        "dropout = 0.2\n",
        "\n",
        "model = AttentiveFPEncoderClassifier(\n",
        "    in_channels=in_channels,\n",
        "    edge_dim=edge_dim,\n",
        "    hidden_channels=hidden_channels,\n",
        "    num_layers=num_layers,\n",
        "    num_timesteps=num_timesteps,\n",
        "    dropout=dropout\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()  # works directly on logits\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "# ======================================\n",
        "# Step 1 — Train AttentiveFP classifier\n",
        "# ======================================\n",
        "def run_epoch(loader, train: bool = True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        logits = model(data.x, data.edge_index, data.edge_attr, data.batch)  # [B,1]\n",
        "        loss = criterion(logits, data.y)\n",
        "\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "\n",
        "        probs = torch.sigmoid(logits).detach().cpu().view(-1).numpy()\n",
        "        labels = data.y.detach().cpu().view(-1).numpy()\n",
        "\n",
        "        all_probs.append(probs)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_probs = np.concatenate(all_probs)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Metrics\n",
        "    try:\n",
        "        roc = roc_auc_score(all_labels, all_probs)\n",
        "    except ValueError:\n",
        "        roc = np.nan\n",
        "\n",
        "    preds = (all_probs > 0.5).astype(int)\n",
        "    acc = accuracy_score(all_labels, preds)\n",
        "    f1 = f1_score(all_labels, preds)\n",
        "    kappa = cohen_kappa_score(all_labels, preds)\n",
        "    precision = precision_score(all_labels, preds)\n",
        "    recall = recall_score(all_labels, preds)\n",
        "    cm = confusion_matrix(all_labels, preds)\n",
        "\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    return avg_loss, acc, roc, f1, kappa, precision, recall, cm\n",
        "\n",
        "num_epochs = 30  # increase if needed\n",
        "best_roc = -1.0\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_acc, train_roc, train_f1, train_kappa, train_prec, train_rec, _ = run_epoch(\n",
        "        train_loader, train=True\n",
        "    )\n",
        "    test_loss, test_acc, test_roc, test_f1, test_kappa, test_prec, test_rec, test_cm = run_epoch(\n",
        "        test_loader, train=False\n",
        "    )\n",
        "\n",
        "    if not np.isnan(test_roc) and test_roc > best_roc:\n",
        "        best_roc = test_roc\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:03d} | \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train ROC-AUC: {train_roc:.4f} | \"\n",
        "        f\"Test Loss: {test_loss:.4f}, Test ROC-AUC: {test_roc:.4f}, \"\n",
        "        f\"F1: {test_f1:.4f}, Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}\"\n",
        "    )\n",
        "    print(\"Test Confusion Matrix:\\n\", test_cm)\n",
        "\n",
        "print(\"\\nBest Test ROC-AUC (AttentiveFP-only):\", best_roc)\n",
        "\n",
        "# ==================================================\n",
        "# Step 1b — Extract molecular embeddings from model\n",
        "# ==================================================\n",
        "def get_embeddings(loader):\n",
        "    model.eval()\n",
        "    all_embs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            logits, emb = model(\n",
        "                data.x, data.edge_index, data.edge_attr, data.batch, return_emb=True\n",
        "            )\n",
        "            all_embs.append(emb.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    X = np.vstack(all_embs)               # [num_mols, hidden_channels]\n",
        "    y = np.vstack(all_labels).ravel()     # [num_mols]\n",
        "    return X, y\n",
        "\n",
        "print(\"\\nExtracting embeddings for train and test sets...\")\n",
        "X_train, y_train = get_embeddings(train_loader)\n",
        "X_test, y_test = get_embeddings(test_loader)\n",
        "\n",
        "print(\"Embedding shapes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# ======================================\n",
        "# Scale embeddings (important for SVM)\n",
        "# ======================================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# =====================================================\n",
        "# Step 2 — Optuna tuning for SVM on top of embeddings\n",
        "# =====================================================\n",
        "\n",
        "# Split train embeddings into inner-train and validation\n",
        "X_train_svm, X_val_svm, y_train_svm, y_val_svm = train_test_split(\n",
        "    X_train_scaled, y_train,\n",
        "    test_size=0.2,\n",
        "    stratify=y_train,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "def svm_objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    C = trial.suggest_float(\"C\", 1e-2, 1e3, log=True)\n",
        "    gamma = trial.suggest_float(\"gamma\", 1e-4, 1e1, log=True)\n",
        "    kernel = trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\"])\n",
        "\n",
        "    svm = SVC(\n",
        "        kernel=kernel,\n",
        "        C=C,\n",
        "        gamma=gamma if kernel == \"rbf\" else \"scale\",\n",
        "        probability=True,\n",
        "        class_weight='balanced'\n",
        "    )\n",
        "\n",
        "    svm.fit(X_train_svm, y_train_svm)\n",
        "\n",
        "    y_val_proba = svm.predict_proba(X_val_svm)[:, 1]\n",
        "\n",
        "    try:\n",
        "        val_roc = roc_auc_score(y_val_svm, y_val_proba)\n",
        "    except ValueError:\n",
        "        val_roc = 0.5  # fallback if something goes wrong\n",
        "\n",
        "    return val_roc\n",
        "\n",
        "print(\"\\nRunning Optuna hyperparameter search for SVM...\")\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(svm_objective, n_trials=30)  # increase n_trials for deeper search\n",
        "\n",
        "print(\"Best SVM params:\", study.best_params)\n",
        "print(\"Best validation ROC-AUC from Optuna:\", study.best_value)\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "# Train final SVM on full training embeddings with best params\n",
        "final_kernel = best_params[\"kernel\"]\n",
        "final_C = best_params[\"C\"]\n",
        "final_gamma = best_params[\"gamma\"] if final_kernel == \"rbf\" else \"scale\"\n",
        "\n",
        "best_svm = SVC(\n",
        "    kernel=final_kernel,\n",
        "    C=final_C,\n",
        "    gamma=final_gamma,\n",
        "    probability=True,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "print(\"\\nTraining final SVM with best hyperparameters on full training embeddings...\")\n",
        "best_svm.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate on held-out test set\n",
        "y_proba = best_svm.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred = best_svm.predict(X_test_scaled)\n",
        "\n",
        "svm_roc = roc_auc_score(y_test, y_proba)\n",
        "svm_f1 = f1_score(y_test, y_pred)\n",
        "svm_acc = accuracy_score(y_test, y_pred)\n",
        "svm_kappa = cohen_kappa_score(y_test, y_pred)\n",
        "svm_prec = precision_score(y_test, y_pred)\n",
        "svm_rec = recall_score(y_test, y_pred)\n",
        "svm_cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"\\n===== SVM on AttentiveFP Embeddings (Optuna-tuned) =====\")\n",
        "print(f\"ROC-AUC   : {svm_roc:.4f}\")\n",
        "print(f\"F1        : {svm_f1:.4f}\")\n",
        "print(f\"Accuracy  : {svm_acc:.4f}\")\n",
        "print(f\"Kappa     : {svm_kappa:.4f}\")\n",
        "print(f\"Precision : {svm_prec:.4f}\")\n",
        "print(f\"Recall    : {svm_rec:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", svm_cm)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from attentiveFP alone\n",
        "\n",
        "\"Train Accuracy: 0.86\n",
        "Test Accuracy 0.8125\n",
        "ROC-AUC 0.8134\n",
        "F1-Score 0.8108\n",
        "Cohen's Kappa 0.6256\n",
        "Precision 0.8851\n",
        "Recall 0.9167 \""
      ],
      "metadata": {
        "id": "JaAbeH3bpwMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6UWIhcOESwf"
      },
      "outputs": [],
      "source": [
        "===== SVM on AttentiveFP Embeddings (Optuna-tuned) ===== AID 932\n",
        "ROC-AUC   : 0.8463\n",
        "F1        : 0.7928\n",
        "Accuracy  : 0.7688\n",
        "Kappa     : 0.5355\n",
        "Precision : 0.7299\n",
        "Recall    : 0.8676\n",
        "Confusion Matrix:\n",
        " [[261 131]\n",
        " [ 54 354]]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}